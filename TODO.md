* Intro
* Analyses
    * Three-component data, too?
    * number of b/p tokens per day (based on SEEDlings data at 6months)
      https://github.com/ebergelson/sixmonth_seedlings_paper/tree/master/data
      it's something like 60 and 140 a day for /b/ and /p/ respectively (but
      there's some variability 
* Results
    * text for results
* Figure out "the point"
    * Something like you can get pretty good approximation of ideal inference
      with a psychologically reasonable model.
    * this inference is surprisingly efficient: only takes a few thousands of
      observations
    * yes, we're approximating rational inference. BUT if the approximations
      place similar constraints as actual learners face, then we can get some
      insight into how people actually can do this.
        * e.g., a slower learning rate than if you had all teh data at once
        * forgetting history can be a problem but it can also HELP you
    * Also, we know that people CAN do this.  can help improve the algorithms if
      we know how they fail on tasks that people can do.  e.g. might need to be
      able to revise hypotheses (e.g. merge categories) in ways that correct for
      some of the history-forgetting process that happens.
