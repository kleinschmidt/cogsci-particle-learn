---
title: "Learning distributions as they come: Particle filter models for online
  distributional learning of phonetic categories"
author: 
- name: "Dave F. Kleinschmidt"
  email: "davidfk@princeton.edu"
  affiliation:
  - "Princeton Neuroscience Institute"
  - "Princeton, NJ 08544 USA"
abstract: >
  Babies must learn the structure of their native language---including the
  number and properties of phonetic categories. Blah BLah.
bibliography: /home/dave/Documents/papers/zotero.bib
---

```{julia; echo=false; results="hide"}

using Weave
Weave.set_chunk_defaults(Dict(:echo=>false,
                              :results=>"hide"))

using Particles
using Underscore

using Distributions, ConjugatePriors, DataFrames, DataFramesMeta, JLD2
using Gadfly

using Colors
ngray(start, finish) = n -> linspace(Gray(start), Gray(finish), n)

```



# Bayesian models of distributional learning

Distributional learning models all attempt to solve the same problem: given some
data points $x_{1:N}$, we wish to know how many clusters generated them (and
what those clusters look like).  We assume that these data were generated from
some number of clusters $K$, where $K$ could be (in principle) anywhere between
1 and $N$ (the number of data points).  A complete clustering of these data
points has two parts: the cluster that each data point is assigned to (denoted
$c_{1:N}$), and the properties of the clusters themselves (which in the example
below are the mean $\mu_k$ and variance $\sigma^2_k$ of a normal distribution,
but in general are the parameters of some probability distribution).  These
models allow for a possibly infinite number of clusters, but have an inductive
bias towards simpler, more parsimonious clusterings.

The first difficulty is that there's no perfect, un-ambiguous solution to this
problem (in general), since clusters are often overlapping and of dfiferent
sizes and frequencies.  For this reason, Bayesian nonparametric models frame
this as a problem of _statistical inference under uncertainty_, and aim to find
the posterior probability of clusterings given some particular data, $p(c_{1:N},
\mu_{1:K}, \sigma^2_{1:K} | x_{1:N})$ [see @Gershman2012 for an excellent
introduction].  In many cases, it's easier to do this in two stages: first,
compute the distribution of cluster assignments $p(c_{1:N} | x_{1:N})$
(marginalizing over possible clusters _parameters_), and then later computing
the distribution over cluster properties _conditional_ on the cluster
assignments $p(\mu_{1:K}, \sigma^2_{1:K} | c_{1:N}, x_{1:N})$.[^conjugate] Here
the focus is on the first part: computing the distribution of cluster
assignments given the data.

[^conjugate]: One reason for this is that often when using a conjugate prior for
    the cluster parameters, there's no need to actually know the exact
    properties of cluster to evaluate how good it is; all that's needed is the
    data points that are assigned to that cluster (and some measure of how
    similar they are to each other).

In principle, computing the posterior distribution $p(c_{1:N} | x_{1:N})$ is a
simple matter of applying Bayes Rule:
$$
    p(c_{1:N} | x_{1:N}) \propto p(x_{1:N} | c_{1:N}) p(c_{1:N})
$$
The first term is the likelihood, or the probability that the observed data is
generated by a particular hypothetical clustering.  The second term is the
prior, or how likely such a clustering is before any data is observed.  The
prior is where inductive biases for simpler clustering can be introduced.  

The particular prior used here is the Dirichlet process.  This prior has a
rich-get-richer structure, which is most easily understood by considering the
conditional prior $p(c_n | c_{1:n-1})$ over cluster assignments for the $n$th
data point, given assignments 1 to $n-1$.  The prior probability of assigning to
a cluster $k$ is
$$
    p(c_n=k | c_{1:n-1}) \propto \left\{ 
        \begin{array}{ll}
            N_k    & \textrm{existing cluster} \\
            \alpha & \textrm{new cluster}
        \end{array}\right.
$$
where $N_k$ is the the number of other data points assigned to cluster $k$.  The
$\alpha$ parameter is called the concentration parameter and controls how strong
the simplicity bias is.  If $\alpha$ is very low, it's highly unlikely (a
priori) that a new cluster will be created, especially when there are many data
points.  The overall prior probability of any complete clustering $c_{1:N}$ can
be computed from these conditional probabilities under the assumption that the
data is _exchangeable_ (that order doesn't matter, or that the cluster structure
is stable over time), and is proportional to $\prod_{2=1}^{N} p(c_i | c_{1:i-1})
\propto \prod_{k=1}^K \alpha N_k!$ (arbitrarily defining $c_1 = 1$).


The second difficulty is that there are an enormous number of possible
combinations of cluster assignments for $N$ points, $c_{1:N}$.  Even if we know
that there are only $K=2$ clusters, there are still $2^N$ possible ways to
assign $N$ points to two clusters, each of which has some probability associated
with it.  Likewise for every $K$ from 1 to $N$.  Luckily, most of these values
for $c_{1:N}$ have probability that is so small it's essentially zero, and thus
the whole distribution $p(c_{1:N} | x_{1:N})$ can be approximated by a
reasonably small number of _samples_, or hypothetical values of $c_{1:N}$.

## Batch algorithm: Gibbs sampler

One standard method of doing approximate inference by sampling is a Gibbs
sampler, a form of Markov Chain Monte Carlo (MCMC) techniques.  These are named
because they work by sampling (the "Monte Carlo" part) a new value for the
quantity of interest given only the data and the previously sampled value (the
"Markov Chain" part).

For a Dirichlet Process mixture model, this algorithm works by sweeping through
the data, one data point at a time, re-sampling the cluster assignment for that
data point conditioned on the other data points.  If $c_{-i}$ are the cluster
assignments for every point but $x_i$, then the Gibbs sampler will assign $x_i$
to cluster $k$ with probability $p(c_i=k | x_{1:n}, c_{-i}) \propto p(x_i |
c_i=k, x_{-i}, c_{-i}) p(c_i=k | c_{-i})$---that is, proportional to the
likelihood of $x_i$ given the other data points in cluster $k$ times the prior
probability of $k$ under the Chinese Restaurant Process prior.  The likelihood
for a new category is based on the prior for the category parameters (e.g., mean
and variance).  Once new assignments have been sampled for every $x_i$, the new
values of $c_i$ are one sample from the posterior.  Multiple samples are drawn
by repeatedly sweeping through the data in this way, recording the sampled
assignments for each sweep.

## Online algorithm: Particle filter

In contrast to MCMC algorithms, sequential Monte Carlo (SMC) algorithms do not
require that all data be available simultaneously.

# Methods

In order to evaluate the particle filter as a model of phonetic distributional
learning, I applied it to the problem of learning the English distinction
between voiced /b/ and voiceless /p/, based on voice onset time (VOT).  This is
the primary acoustic cue to voicing for word-initial stops in English
[@Lisker1964], and exhibits a clear bimodality.  In order to simulate random VOT
datasets, I fit a normal distribution to the VOTs for /b/ and /p/ from the
Buckeye corpus [@Pitt2007] by @Nelson2017.  This particular corpus shows low
levels of talker variability in VOT [see @Kleinschmidta].

```{julia; fig_cap="Example sample of VOTs used to assess distributional learning models"}

include("VOTs.jl")
using .VOTs

srand(1986)
plot(layer(x=rand(bp_mix, 10000), Geom.histogram(density=true, bincount=30)),
     Guide.xlabel("Voice onset time (VOT, ms)"))           

```


```{julia}

# include("Experiments.jl")
# using .Experiments

# I had to make an actual honest-to-god package to make this work with JLD2...
using Experiments

using FileIO

# exs, results = load("results/run2-2018-01-30T10:05:12.518.jld2", "exs", "results")
# gibbs_exs, gibbs_res = load("results/run1_gibbs_2018-01-30T12:18:31.076.jld2", "gibbs_exs", "gibbs_res")

@load "results/run2-2018-01-30T10:05:12.518.jld2" exs results
@load "results/run1_gibbs_2018-01-30T12:18:31.076.jld2" gibbs_exs gibbs_res

run2_ers = flatten(exs, results, :num_obs)
run2_summary = results_summary(run2_ers)

gibbs_summary = results_summary(zip(gibbs_exs, gibbs_res))

```

```{julia; label="pK2"; fig_cap="One measure of success is the posterior probability of two-cluster solutions."}
plot(run2_summary, x=:num_obs, y=:avg_p_K2, color=:num_particle, xgroup=:α,
     Geom.subplot_grid(Geom.line, Geom.point), Scale.x_log10, Scale.color_discrete)
```


```{julia; label="meanK"; fig_cap="The expected number of clusters, averaged over runs."}
plot(run2_summary, x=:num_obs, y=:avg_K_mean, color=:num_particle, xgroup=:α,
     Geom.subplot_grid(Geom.line, Geom.point), Scale.x_log10, Scale.y_log2, Scale.color_discrete)
```



```{julia; label="pK"; fig_cap="The posterior probability assigned to each possible number of components, as a function of the number of observations, number of particles, and α"}

# plot p(K | x, α) for various values of alpha

function p_of_K(d)
    pks = d[:p_of_K]
    max_k = maximum(length.(pks))
    ps = zeros(Float64, max_k)
    for pk in pks
        ps[1:length(pk)] .+= pk
    end
    DataFrame(K=1:max_k, p_of_K = ps ./ size(d,1))
end

run2_p_of_K = @_ run2_ers |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

gibbs_p_of_K = @_ zip(gibbs_exs, gibbs_res) |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

@_ @where(gibbs_p_of_K, :num_obs .== 1000, :num_particle .== 1000) |>
    @transform(_, num_particle_ = "Gibbs") |>
    vcat(_, @transform(run2_p_of_K, num_particle_ = string.(:num_particle))) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α))) |>
    @where(_, :num_particle_ .!= "1", :α .!= 10, :K .< 9) |>
    plot(_, x=:K, y=:p_of_K, ygroup=:num_particle_, xgroup=:α_str, color=:num_obs,
         Geom.subplot_grid(Geom.line, Guide.xticks(orientation=:horizontal)), 
         Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]), 
         Guide.colorkey(title="Observations"), 
         Guide.ylabel("Probability of cluster number \n(by number of particles)", :vertical))

```
