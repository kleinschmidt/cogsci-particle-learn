---
title: "Learning distributions as they come: Particle filter models for online
  distributional learning of phonetic categories"
author: 
- name: "Dave F. Kleinschmidt"
  email: "davidfk@princeton.edu"
  affiliation:
  - "Princeton Neuroscience Institute"
  - "Princeton, NJ 08544 USA"
abstract: >
  Babies must learn the structure of their native language---including the
  number and properties of phonetic categories. Blah BLah.
bibliography: /home/dave/Documents/papers/zotero.bib
---

```{julia; echo=false; results="hide"}

using Weave
Weave.set_chunk_defaults(Dict(:echo=>false,
                              :results=>"hide"))

using Particles
using Underscore

using Distributions, ConjugatePriors, DataFrames, DataFramesMeta, JLD2
using Gadfly

using Colors
ngray(start, finish) = n -> linspace(Gray(start), Gray(finish), n)

```

Any healthy, hearing human infant can learn any human language, and they can do
so without requiring much explicit instruction or guidance.  How is this
possible?  At a basic level, how can an infant figure out that their language
has, for instance, two different phonetic categories distinguish by voicing
(e.g., "beach" vs. "peach"), and not three, or just one [@Lisker1964]?  One clue
comes from the fact that sounds from the same category tend to sound alike,
moreso than sounds from different categories.  Infants (and adults) are
sensitive to this cluster structure, and even in the absence of explicit cues or
instructions learn to distinguish between two sounds when they occur in
two different clusters than when they occur in a single, unimodal cluster
[@Maye2002].

There are a number of different models for this "distributional learning", which
fall into two broad families.  On the one hand, there are _computational-level_
models [in the sense of @Marr1972], which focus on the nature of the problem to
be solved, the information that is available from the world, and the best
performance that is possible given the combination of those two factors [e.g.,
@Feldman2013].  On the other hand, there are cognitive, _algorithmic-level_
models, which focus on psychologically plausible representations and processes
[e.g., @McMurray2009b; @Vallabha2007].  Both of these approaches have provided
insight into the process of distributional learning.  Computational-level models
set the boundaries on what is possible _in principle_.  A notable example is the
work of @Feldman2013 which shows, somewhat counterintuitively, that
distributional learning of phonetic categories can in general be _enhanced_ by
simultaneous distributional learning of words.  However, the implementations of
these models are often profoundly psychologically implausible, and may assume
that learners have simultaneous access to the entire batch of data to learn
from, can make multiple passes over that data, or maintain substantial
uncertainty.

Algorithmic-level models, on the other hand, serve as existence proofs that
distributional learning is possible, given particular _representational_
assumptions.  These models often make ad hoc assumptions in order to better fit
behavioral data, as in @McMurray2009 who conclude that "winner-take-all"
competetive dynamics are necessary for distributional learning.  They also
generally lack the in-principle guarantees of computational-level models, and
thus it is unclear whether any particular model's failure or success reflects
fundamental constraints or representational assumptions [though
computational-level models are not immune to this problem; @Goldwater2009].

Bridging these levels of analysis is critical for a comprehensive understanding
of human cognition.  One way to approach this bridge is the family of models
known as rational approximations of rational models [@Sanborn2010].  These
models provide a principled link between computational-level concerns about the
structure of the world and the nature of the tasks that the mind must solve, and
algorithmic-level concerns about psychologically plausible representations and
processes.  In this paper, I explore a psychologically plausible, approximately
rational model for phonetic distributional learning: particle filters for
Bayesian non-parametric clustering.  As a case study, I apply this model to
English stop voicing (e.g., /b/ vs. /p/), and investigate how the performance of
this model is affected by the constraints of online processing (one data point
at a time) and limited representational resources, relative to an unconstrained
distributional learning algorithm.

To start, I review the Bayesian approach to distributional learning as a problem
of statistical inference under uncertainty.  Next, I briefly summarize the two
algorithms for doing this inference, a Gibbs sampler batch algorithm, and a
particle filter online algorithm.  I then analyze simulations of these two
models on the same (simulated) phonetic cue distributions, focusing on how well
they can recover the true underlying structure, before discussing the
implications of these results.

# Bayesian models of distributional learning

Distributional learning models all attempt to solve the same problem: given some
data points $x_{1:N}$, we wish to know how many clusters generated them (and
what those clusters look like).  We assume that these data were generated from
some number of clusters $K$, where $K$ could be (in principle) anywhere between
1 and $N$ (the number of data points).  A complete clustering of these data
points has two parts: the cluster that each data point is assigned to (denoted
$c_{1:N}$), and the properties of the clusters themselves (which in the example
below are the mean $\mu_k$ and variance $\sigma^2_k$ of a normal distribution,
but in general are the parameters of some probability distribution).  These
models allow for a possibly infinite number of clusters, but have an inductive
bias towards simpler, more parsimonious clusterings.

The first difficulty is that there's no perfect, un-ambiguous solution to this
problem (in general), since clusters are often overlapping and of dfiferent
sizes and frequencies.  For this reason, Bayesian nonparametric models frame
this as a problem of _statistical inference under uncertainty_, and aim to find
the posterior probability of clusterings given some particular data, $p(c_{1:N},
\mu_{1:K}, \sigma^2_{1:K} | x_{1:N})$ [see @Gershman2012 for an excellent
introduction].  In many cases, it's easier to do this in two stages: first,
compute the distribution of cluster assignments $p(c_{1:N} | x_{1:N})$
(marginalizing over possible clusters _parameters_), and then later computing
the distribution over cluster properties _conditional_ on the cluster
assignments $p(\mu_{1:K}, \sigma^2_{1:K} | c_{1:N}, x_{1:N})$.[^conjugate] Here
the focus is on the first part: computing the distribution of cluster
assignments given the data.

[^conjugate]: One reason for this is that often when using a conjugate prior for
    the cluster parameters, there's no need to actually know the exact
    properties of cluster to evaluate how good it is; all that's needed is the
    data points that are assigned to that cluster (and some measure of how
    similar they are to each other).

In principle, computing the posterior distribution $p(c_{1:N} | x_{1:N})$ is a
simple matter of applying Bayes Rule:
$$
    p(c_{1:N} | x_{1:N}) \propto p(x_{1:N} | c_{1:N}) p(c_{1:N})
$$
The first term is the likelihood, or the probability that the observed data is
generated by a particular hypothetical clustering.  The second term is the
prior, or how likely such a clustering is before any data is observed.  The
prior is where inductive biases for simpler clustering can be introduced.  

The particular prior used here is the Dirichlet process.  This prior has a
rich-get-richer structure, which is most easily understood by considering the
conditional prior $p(c_n | c_{1:n-1})$ over cluster assignments for the $n$th
data point, given assignments 1 to $n-1$.  The prior probability of assigning to
a cluster $k$ is
$$
    p(c_n=k | c_{1:n-1}) \propto \left\{
        \begin{array}{ll}
            N_k    & \textrm{existing cluster} \\
            \alpha & \textrm{new cluster}
        \end{array}\right.
$$
where $N_k$ is the the number of other data points assigned to cluster $k$.  The
$\alpha$ parameter is called the concentration parameter and controls how strong
the simplicity bias is.  If $\alpha$ is very low, it's highly unlikely (a
priori) that a new cluster will be created, especially when there are many data
points.  The overall prior probability of any complete clustering $c_{1:N}$ can
be computed from these conditional probabilities under the assumption that the
data is _exchangeable_ (that order doesn't matter, or that the cluster structure
is stable over time), and is proportional to $\prod_{2=1}^{N} p(c_i | c_{1:i-1})
\propto \prod_{k=1}^K \alpha N_k!$ (arbitrarily defining $c_1 = 1$).


The second difficulty is that there are an enormous number of possible
combinations of cluster assignments for $N$ points, $c_{1:N}$.  Even if we know
that there are only $K=2$ clusters, there are still $2^N$ possible ways to
assign $N$ points to two clusters, each of which has some probability associated
with it.  Likewise for every $K$ from 1 to $N$.  Luckily, most of these values
for $c_{1:N}$ have probability that is so small it's essentially zero, and thus
the whole distribution $p(c_{1:N} | x_{1:N})$ can be approximated by a
reasonably small number of _samples_, or hypothetical values of $c_{1:N}$.

## Batch algorithm: Gibbs sampler

One standard method of doing approximate inference by sampling is a Gibbs
sampler, a form of Markov Chain Monte Carlo (MCMC) techniques.  These are named
because they work by sampling (the "Monte Carlo" part) a new value for the
quantity of interest given only the data and the previously sampled value (the
"Markov Chain" part).

For a Dirichlet Process mixture model, this algorithm works by sweeping through
the data, one data point at a time, re-sampling the cluster assignment for that
data point conditioned on the other data points.  If $c_{-i}$ are the cluster
assignments for every point but $x_i$, then the Gibbs sampler will assign $x_i$
to cluster $k$ with probability $p(c_i=k | x_{1:N}, c_{-i}) \propto p(x_i |
c_i=k, x_{-i}, c_{-i}) p(c_i=k | c_{-i})$---that is, proportional to the
likelihood of $x_i$ given the other data points in cluster $k$ times the prior
probability of $k$ under the Chinese Restaurant Process prior.  The likelihood
for a new category is based on the prior for the category parameters (e.g., mean
and variance).  Once new assignments have been sampled for every $x_i$, the new
values of $c_i$ are one sample from the posterior.  Multiple samples are drawn
by repeatedly sweeping through the data in this way, recording the sampled
assignments for each sweep.

## Online algorithm: Particle filter

In contrast to MCMC algorithms, sequential Monte Carlo (SMC) algorithms do not
require that all data be available simultaneously.  Rather then generating
samples one at a time based on the entire dataset, they maintain a population of
hypotheses (or particles), that are each updated in parallel as the data comes
in.  After $n-1$ observations, particle $j$ has an associated weight
$w^{(j)}_{n-1}$ and clustering $c_{1:n-1}^{(j)}$.  There are many different
strategies for updating particle $j$ based on the the next observation $x_n$.
Here, we follow the approach of [@Chen2000; as described in @Fearnhead2004].
First, as in the Gibbs sampler, an assignment is drawn from $p(c_n=k |
c^{(j)}_{1:n-1}, x_{1:n})$.  Next, the weight is updated to be
$$
    w_{n}^{(j)} = w_{n-1}^{(j)} \times
        \frac{\sum_k p((c_{1:n-1}^{(j)},k) | x_{1:n})}
             {p(c_{1:n-1}^{(j)} | x_{1:n-1})} 
$$
(normalized such that all the new weights sum to 1).  The sum in the numerator
ensures that the new weight reflects the ability of this particle to predict the
actual data point observed, rather than just how well the sampled component
explains it [see @Chen2000; @Fearnhead2004 for further discussion].

One common problem with particle filters is that a small number of particles
capture nearly all the weight, which drastically reduces the effective number of
samples and hence the amount of information about the actual distribution they
are approximating.  In order to prevent this, when the variance of the weights
becomes too high, a rejuvination step resamples particles (with replacement)
proportional to their weights, and resets the weights to be equal.  The
threshold for the variance of the weights was set to 50% of the mean of the
weights [as suggested by @Fearnhead2004].

# Methods

In order to evaluate the particle filter as a model of phonetic distributional
learning, I applied it to the problem of learning the English distinction
between voiced /b/ and voiceless /p/, based on voice onset time (VOT).  This is
the primary acoustic cue to voicing for word-initial stops in English
[@Lisker1964], and exhibits a clear bimodality.  In order to simulate random VOT
datasets, I fit a normal distribution to the VOTs for /b/ and /p/ from the
Buckeye corpus [@Pitt2007] by @Nelson2017.  This particular corpus shows low
levels of talker variability in VOT [see @Kleinschmidta].  @Fig:vothist shows an
example randomly generated set of VOTs.

```{julia; label="vothist"; fig_cap="Example sample of VOTs used to assess distributional learning models.  With fewer observations, the cluster structure is not obvious, but with more clusters it becomes clearer."}

include("VOTs.jl")
using .VOTs

srand(1986)
vot_demo = DataFrame(vot = rand(bp_mix, 10000),
                     n = append!([10], round.(Int, exp10.(ceil.(log10.(2:10000))))))
plot(layer(vot_demo, x=:vot, xgroup=:n, Geom.subplot_grid(Geom.histogram(density=true, bincount=30))),
     # Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]),
     Guide.xlabel("Voice onset time (VOT, ms)"))

```

The Gibbs sampler and particle filter models were implemented in Julia
[@Bezanson2017], and run on 200 randomly generated data sets.[^particlesjl]  For
each dataset, the number of particles (or samples for the Gibbs sampler), the
number of observations available, and the concentration parameter $\alpha$ were
varied.  The results for the Gibbs sampler were qualitatively the same across
the number of samples and number of observations, so results are only shown for
1,000 samples (500 burnin) and 1,000 observations.

For the prior distribution over cluster means and variances, both used a weakly
informative conjugate Normal-$\chi^{-2}$ prior [@Gelman2003], with $\mu_0$ and
$\sigma^2_0$ set to the overall mean and variance of all the VOTs, and $\kappa_0
= 0.05$ (to allow for significant possible variation in the cluster means) and
$\nu_0 = 2$ (to constrain the variances to reasonable values).

[^particlesjl]: Available online at
    [github.com/kleinschmidt/Particles.jl](https://github.com/kleinschmidt/Particles.jl)

For each run, the approximated posterior distribution of cluster numbers is
recorded.  In the case of the Gibbs sampler, this is simply the proportion of
samples with each number of clusters $K$.  The calculation is analogous for the
particle filter, except that the weights need to be taken into account.  If
particle $j$ has $K^{(j)}$ clusters and weight $w^{(j)}$, then $p(K=k | x_{1:N})
= \sum_j w^{(j)} \delta_{k}(K^{(j)})$ (where $\delta_k(x)$ is the indicator
function, which is 1 if $x=k$ and 0 otherwise).

# Results

A natural measure of success is the probability assigned to $K=2$, since the
data was generated by a two-component mixture (@Fig:pK2).  I also plot the
expected number of clusters $E(K | x_{1:N}) = \sum_k k p(K=k | x_{1:N})$
(@Fig:meanK), as well as the full distributions ($p(K | x_{1:N})$, @Fig:pK).


```{julia}

# include("Experiments.jl")
# using .Experiments

# I had to make an actual honest-to-god package to make this work with JLD2...
# Pkg.clone("git@github.com:kleinschmidt/Experiments.jl.git")
using Experiments

@load "results/run2-2018-01-30T10:05:12.518.jld2" exs results
@load "results/run1_gibbs_2018-01-30T12:18:31.076.jld2" gibbs_exs gibbs_res

run2_ers = flatten(exs, results, :num_obs)
run2_summary = @where(results_summary(run2_ers), :num_particle .> 1)

gibbs_summary = @_ zip(gibbs_exs, gibbs_res) |>
    results_summary |>
    @where(_, :num_obs .== 1000, :num_particle .== 1000)
```

## Gibbs sampler

Since the Gibbs sampler provides the "reference" approximation for these models,
we first examine those results, shown as the gray lines in @Fig:pK2 and
@Fig:meanK.  The Gibbs sampler performs best with $\alpha=0.1$, allocating the
majority of the posterior probability to $K=2$.  For $\alpha$ an order of
magnitude smaller or larger than this, the Gibbs sampler infers fewer or more
(respectively) clusters than there actually are (@Fig:meanK).

## Particle filter

The particle filter, unlike the Gibbs sampler, works best when $\alpha=0.01$
(the smallest value tested).  For larger values, it eventually overshoots and
infers more than 2 clusters.  For instance, @Fig:pK2, second panel
($\alpha=0.1$), shows that the probability assigned to $K=2$ clusters rises to a
maximum around 100 data points, but then falls with more data as more complex
clusterings are increasingly preferred (@Fig:pK).  Even though the expected number
of clusters is just slightly more than 2 (@Fig:meanK) as with the Gibbs sampler,
there's no reason to think that it would continue to increase with more data,
since there's no way for the particle filter to go back to simpler solutions
once they've been forgotten.

Generally, particle filters with more particles better match the Gibbs sampler.
This is not surprising: more particles mean more tolerance for uncertainty,
which means that the particle filter is less committed to particular
classifications that it made in the past.  With few particles, it's possible
(and indeed likely) that most of the particles agree on how the first data
points should be classified, even if there is (ideally) some uncertainty there.
Furthermore, with few particles, the prior has outsized influence.  For low
$\alpha$, the 10-particle filter undershoots the number of clusters inferred by
the Gibbs sampler (and by more particles), while for high $\alpha$ it
overshoots.

<!-- points to make:

* works well for v. small alpha (0.01), and surprisingly quickly.
* particle filter is better approximation (closer to the gibbs sampler) with more
  observations and more particles.
* when there are fewer particles, seem to get exaggerated influence of prior
  (alpha): more bias towards small K for v. low, runs away at higher \alpha.
* except for v. small values of alpha
* problem is that it can't keep the full uncertainty about the clustering "in
  memory", because it can't go back and revise previous classifications (the
  data is gone).  _that might not be a bad thing for psycological
  plausibility!_.  And actually may work in favor for smaller learning rates.
* Also can never go back to simpler solutions once they've fallen out of
  consideration.  This may be why for large \alpha the number of components runs
  away.
* likewise why it works better for small alpha: can't re-visit the decision to
  create a second cluster, but it'll be hard to do it again in the future
  (because prior scales with $\alpha/N$.

 -->

```{julia; label="pK2"; fig_cap="Posterior probability correct (two-cluster solutions) for particle filter (colors) and Gibbs sampler (gray horizontal lines). Gibbs sampler is shown for 1,000 observations and 1,000 samples."}

plot(run2_summary, xgroup=:α,
     Geom.subplot_grid(layer(run2_summary, 
                             x=:num_obs, y=:avg_p_K2, color=:num_particle, xgroup=:α,
                             Geom.line, Geom.point),
                       layer(gibbs_summary,
                             yintercept=:avg_p_K2, xgroup=:α,
                             Geom.hline,
                             Theme(default_color=Gray(0.5)))),
     Scale.x_log10, Scale.color_discrete,
     Guide.ylabel("Average probability of correct (K=2)"),
     Guide.xlabel("Number of observations"),
     Guide.colorkey(title="Particles"),
     Guide.manual_color_key("", ["Gibbs"], [Gray(0.5)]))

```

```{julia; label="meanK"; fig_cap="The expected number of clusters for particle filter (colors) and Gibbs sampler (gray horizontal lines).  Gibbs sampler is shown for 1,000 observations and 1,000 samples"}

plot(run2_summary, xgroup=:α,
     Geom.subplot_grid(layer(run2_summary, 
                             x=:num_obs, y=:avg_K_mean, color=:num_particle, xgroup=:α,
                             Geom.line, Geom.point),
                       layer(gibbs_summary,
                             yintercept=:avg_K_mean, xgroup=:α,
                             Geom.hline,
                             Theme(default_color=Gray(0.5)))),
     Scale.x_log10, Scale.y_log2, Scale.color_discrete,
     Guide.ylabel("Expected number of clusters"),
     Guide.xlabel("Number of observations"),
     Guide.colorkey(title="Particles"),
     Guide.manual_color_key("", ["Gibbs"], [Gray(0.5)]))

```



```{julia; label="pK"; fig_cap="The posterior probability assigned to each possible number of components, as a function of the number of observations, number of particles, and \$\\alpha\$"}

# plot p(K | x, α) for various values of alpha

function p_of_K(d)
    pks = d[:p_of_K]
    max_k = maximum(length.(pks))
    ps = zeros(Float64, max_k)
    for pk in pks
        ps[1:length(pk)] .+= pk
    end
    DataFrame(K=1:max_k, p_of_K = ps ./ size(d,1))
end

run2_p_of_K = @_ run2_ers |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K) |>
    @where(_, :num_particle .> 1)

gibbs_p_of_K = @_ zip(gibbs_exs, gibbs_res) |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

@_ @where(gibbs_p_of_K, :num_obs .== 1000, :num_particle .== 1000) |>
    @transform(_, num_particle_ = "Gibbs") |>
    vcat(_, @transform(run2_p_of_K, num_particle_ = string.(:num_particle))) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α))) |>
    @where(_, :α .!= 10, :K .< 9) |>
    plot(_, x=:K, y=:p_of_K, ygroup=:num_particle_, xgroup=:α_str, color=:num_obs,
         Geom.subplot_grid(Geom.line, Guide.xticks(orientation=:horizontal)), 
         Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]), 
         Guide.colorkey(title="Observations"), 
         Guide.ylabel("Probability of cluster number \n(by number of particles)", :vertical))

```

# Discussion

Rational approximations to rational models---like the particle filter explored
here---provide a natural bridge between computational-level and
algorithmic-level, cognitive models [@Sanborn2010].  These techniques are useful
in part because they approximate optimal statistical inference as specified by
the underlying Bayesian model.  But they also allow us to explore---in a formal,
quantitative way---how different kinds of cognitive constraints interact with
the structure of the world to affect human cognition and learning.  In the case
of phonetic distributional learning, there is no shortage of cognitively
plausible models [e.g., @McMurray2009b; @Vallabha2007].  The model explored here
is both psychologically plausible---it requires maintaining a small, finite
number of hypotheses---and grounded in a computational-level model of the
problem of distributional learning and its statistically optimal solution.

The approximation to the statistically optimal solution to distributional
learning provided by the particle filtering algorithm of @Chen2000 can, in fact,
recover the underlying structure of the particular model system I examined here
(the American English /b/-/p/ contrast).  More importantly, this particular sort
of approximation constrains the model in a similar way that language learners
are constrained: they cannot endlessly re-analyze every single sound they have
ever heard, nor can they maintain an essentially infinite set of hypotheses
about how those sounds should be clustered.  In doing so, it provides some
insight into how people actually solve the statistical problem posed by the
computational-level model of distributional learning.  Forgetting its own
history actually _helps_ the particle filter model when there is a strong bias
towards fewer clusters (low $\alpha$).  The particle filter reliably arrives at
the correct two-cluster structure for low values of $\alpha$, even when the
Gibbs sampler fails to do so, since the Gibbs sampler can continuously
second-guess its decisions to create additional clusters.  The tendency for
particle filters to get dug into a particular solution is often a shortcoming,
but in this case it may be a benefit.  It also suggests that such limited
resources may actually be a benefit to human learners as well.  It does not,
however, require that such limitations _change_ with development [as in
@Newport1990] or require or particularly benefit from a specific input [as in
@Elman1993].  In fact, the constraints imposed by limited tolerance for
uncertainty in the particle filter model examined here are in a sense the
_opposite_ of the "less is more" hypothesis: these constraints _allow_ the
learner to impose stronger a priori preference for simple explanations and still
learn, rather than _constituting_ a simplicity preference in and of themselves.
Indeed, one of the advantages of the kind of cognitively constrained rational
models described here is that it allows a principled exploration of the way that
cognitive constraints interact with different kinds of input and structural
assumptions on the part of the learner [@Rohde1999; @Siegelman2015].

Additionally, specifying and exploring such models of human behavior also has
the potential to improve basic computational techniques as well.  We know that
humans do manage to learn the underlying cluster structure from unsupervised
input like this.  The particular ways in which _models_ fail at this task can be
instructive for creating better models.  This may mean taking into account
higher-order structure where it's present, like phonotactic and lexical
regularities [@Feldman2013].  But it might also motivate specific techniques to
get around the difficulty of moving from more complex to less complex
clusterings, like adding the possibility of merging clusters when rejuvenating
particles [analogously to reversible-jump MCMC, @Green1995].

# Conclusion

Approximately-rational models like the particle filter provide a possible bridge
between computational-level models and psychologically plausible
algorithmic-level cognitive models.  A particle filter model of phonetic
distributional learning is able to learn the underlying cluster structure of the
English /b/-/p/ contrast based only on the distribution of a single cue (VOT).
This shows that it is possible to approximate optimal Bayesian inference in this
domain without making the psychologically-implausible assumptions of batch
processing and unlimited tolerance of uncertainty.  However, the behavior of
this approximation also diverges in potentially interesting ways from a
less-constrained approximate inference model (a Gibbs sampler), suggesting that
the constraints posed by limited cognitive resources are a critical piece of the
puzzle in understanding cognition, even for computational-level modeling
[@Marr1972].
