---
title: "Learning distributions as they come: Particle filter models for online
  distributional learning of phonetic categories"
author: 
- name: "Dave F. Kleinschmidt"
  email: "davidfk@princeton.edu"
  affiliation:
  - "Princeton Neuroscience Institute"
  - "Princeton, NJ 08544 USA"
abstract: >
    Human infants have the remarkable ability to learn any human language.  One
    proposed mechanism for this ability is distributional learning, where
    learners infer the underlying cluster structure from unlabeled input.
    Computational models of distributional learning have historically been
    principled but psychologically-implausible computational-level models, or
    ad hoc but psychologically plausible algorithmic-level models.  Approximate
    rational models like particle filters can potentially bridge this divide,
    and allow principled, but psychologically plausible models of distributional
    learning to be specified and evaluated.  As a proof of concept, I evaluate
    one such particle filter model, applied to learning English voicing
    categories from distributions of voice-onset times (VOTs).  I find that this
    model learns well, but behaves somewhat differently from the standard,
    unconstrained Gibbs sampler implementation of the underlying rational
    model.
keywords: 
- Computational modeling
- Rational models
- Particle filters
- Language learning
- Distributional learning
- Speech perception
bibliography: /home/dave/Documents/papers/zotero.bib
---

```{julia; echo=false; results="hide"}

using Weave
Weave.set_chunk_defaults(Dict(:echo=>false,
                              :results=>"hide"))

using Particles
using Underscore

using Distributions, ConjugatePriors, DataFrames, DataFramesMeta, JLD2
using Gadfly

using Colors
ngray(start, finish) = n -> linspace(Gray(start), Gray(finish), n)

```

Any normally developing human infant can learn any human language, and they can do
so without requiring much explicit instruction or guidance.  How is this
possible?  At a basic level, how can an infant figure out that their language
has, for instance, two different phonetic categories distinguished by voicing
(e.g., "beach" vs. "peach"), and not three, or just one [@Lisker1964]?  One clue
comes from the fact that sounds from the same category tend to sound alike,
more so than sounds from different categories.  Infants (and adults) are
sensitive to this cluster structure, and even in the absence of explicit cues or
instructions learn to distinguish between two sounds better when they occur in
two different clusters than when they occur in a single, unimodal cluster
[@Maye2002].

There are a number of different models for this "distributional learning", which
fall into two broad families.  On the one hand, there are _computational-level_
models [in the sense of @Marr1982], which focus on the nature of the problem to
be solved, the information that is available from the world, and the best
performance that is possible given the combination of those two factors [e.g.,
@Feldman2013].  On the other hand, there are cognitive, _algorithmic-level_
models, which focus on psychologically plausible representations and processes
[e.g., @McMurray2009b; @Vallabha2007].  Both of these approaches have provided
insight into the process of distributional learning.  Computational-level models
set the boundaries on what is possible _in principle_.  A notable example is the
work of @Feldman2013 which shows, somewhat counterintuitively, that
distributional learning of phonetic categories can in general be _enhanced_ by
simultaneous distributional learning of words.  However, the implementations of
these models are often profoundly psychologically implausible, and may assume
that learners have simultaneous access to the entire batch of data to learn
from, can make multiple passes over that data, or maintain unlimited amounts of
uncertainty.

Algorithmic-level models, on the other hand, serve as existence proofs that
distributional learning is possible, given particular _representational_
assumptions.  These models often make ad hoc assumptions in order to better fit
behavioral data, as in @McMurray2009b who conclude that "winner-take-all"
competitive dynamics are necessary for distributional learning.  These models also
generally lack the in-principle guarantees of computational-level models, and
thus it is unclear whether any particular model's failure or success reflects
fundamental constraints or representational assumptions [though
computational-level models are not immune to this problem; @Goldwater2009].

Each level of analysis offers insight, but bridging between these levels is
critical for a comprehensive understanding of human cognition [@Marr1982].  One
way to approach such a bridge is the family of models known as rational
approximations of rational models [@Sanborn2010].  These models provide a
principled link between computational-level concerns about the structure of the
world and the nature of the tasks that the mind must solve, and
algorithmic-level concerns about psychologically plausible representations and
processes.  In this paper, I explore a psychologically plausible, approximately
rational model for phonetic distributional learning: particle filters for
Bayesian non-parametric clustering.  As a case study, I apply this model to
English stop voicing (e.g., /b/ vs. /p/), and investigate how the performance of
this model is affected by the constraints of online processing (one data point
at a time) and limited representational resources, relative to an unconstrained
distributional learning algorithm.

To start, I review the Bayesian approach to distributional learning as a problem
of statistical inference under uncertainty.  Next, I briefly summarize two
algorithms for doing this inference: a batch Gibbs sampler algorithm, and an
online particle filter algorithm.  I then analyze the performance of these two
models on the same (simulated) phonetic cue distributions, focusing on how well
they can recover the true underlying structure, before discussing the
implications of these results.

# Bayesian models of distributional learning

Distributional learning models all attempt to solve the same problem: given some
data points $x_{1:N}$, we wish to know how many clusters generated them (and
what those clusters look like).  We assume that these data were generated from
some number of clusters $K$, where $K$ could be (in principle) anywhere between
1 and $N$ (the number of data points).  A complete clustering of these data
points has two parts: the cluster that each data point is assigned to (denoted
$c_{1:N}$), and the properties of the clusters themselves (which in the example
below are the mean $\mu_k$ and variance $\sigma^2_k$ of a normal distribution,
but in general are the parameters of some probability distribution).

The first difficulty is that there's no perfect, un-ambiguous solution to this
problem (in general), since clusters are often overlapping and of different
sizes and frequencies.  For this reason, Bayesian nonparametric models frame
this as a problem of _statistical inference under uncertainty_, and aim to find
the posterior probability for each possible clustering, given some particular
data, $p(c_{1:N}, \mu_{1:K}, \sigma^2_{1:K} | x_{1:N})$ [see @Gershman2012 for
an excellent introduction].  In many cases, it's easier to do this in two
stages: first, compute the distribution of cluster assignments $p(c_{1:N} |
x_{1:N})$ (marginalizing over possible clusters _parameters_), and then 
compute the distribution over cluster properties, conditional on the cluster
assignments $p(\mu_{1:K}, \sigma^2_{1:K} | c_{1:N}, x_{1:N})$.[^conjugate] Here
the focus is on the first part: computing the distribution of cluster
assignments given the data.

[^conjugate]: One reason for this is that often when using a conjugate prior for
    the cluster parameters, there's no need to actually know the exact
    properties of cluster to evaluate how good it is; all that's needed is the
    data points that are assigned to that cluster (and some measure of how
    similar they are to each other).

In principle, computing the posterior distribution $p(c_{1:N} | x_{1:N})$ is a
simple matter of applying Bayes Rule:
$$
    p(c_{1:N} | x_{1:N}) \propto p(x_{1:N} | c_{1:N}) p(c_{1:N})
$$
The first term is the likelihood, or the probability that the observed data is
generated by a particular hypothetical clustering, which will depend on how
similar the data points are in each cluster, and on the prior distribution over
cluster parameters.  The second term is the prior, or how likely such a
clustering is before any data is observed.  The prior is where inductive biases
for simpler clustering can be introduced.

The particular prior used here is the Dirichlet process.  This prior has a
rich-get-richer structure, which is most easily understood by considering the
conditional prior $p(c_n | c_{1:n-1})$ over cluster assignments for the $n$th
data point, given assignments 1 to $n-1$.  The prior probability of assigning to
a cluster $k$ is
$$
    p(c_n=k | c_{1:n-1}) \propto \left\{
        \begin{array}{ll}
            N_k    & \textrm{existing cluster} \\
            \alpha & \textrm{new cluster}
        \end{array}\right.
$$
where $N_k$ is the the number of other data points assigned to cluster $k$.  The
$\alpha$ parameter is called the concentration parameter and controls how strong
the simplicity bias is.  If $\alpha$ is very low, it's highly unlikely (a
priori) that a new cluster will be created, especially when there are many data
points.  The overall prior probability of any complete clustering $c_{1:N}$ can
be computed from these conditional probabilities under the assumption that the
data is _exchangeable_ (that order doesn't matter, or that the cluster structure
is stable over time), and is proportional to $\prod_{2=1}^{N} p(c_i | c_{1:i-1})
\propto \prod_{k=1}^K \alpha N_k!$ (arbitrarily defining $c_1 = 1$).

The second difficulty is that there are an enormous number of possible
combinations of cluster assignments.  Even if we know that there are only $K=2$
clusters, there are still $2^N$ possible ways to assign $N$ points to two
clusters, each of which has some probability associated with it.  Likewise for
every $K$ from 1 to $N$.  Luckily, most of these values for $c_{1:N}$ have
probability that is so small it's essentially zero, and thus the whole
distribution $p(c_{1:N} | x_{1:N})$ can be approximated by a reasonably small
number of _samples_, or hypothetical values of $c_{1:N}$.

## Batch algorithm: Gibbs sampler

One standard method of doing approximate inference by sampling is a Gibbs
sampler, a form of Markov Chain Monte Carlo (MCMC) techniques.  These are named
because they work by sampling (the "Monte Carlo" part) a new value for the
quantity of interest given only the data and the previously sampled value (the
"Markov Chain" part).

For a Dirichlet Process mixture model, this algorithm works by sweeping through
the data, one data point at a time, re-sampling the cluster assignment for that
data point conditioned on the other data points.  If $c_{-i}$ are the cluster
assignments for every point but $x_i$, then the Gibbs sampler will assign $x_i$
to cluster $k$ with probability $p(c_i=k | x_{1:N}, c_{-i}) \propto p(x_i |
c_i=k, x_{-i}, c_{-i}) p(c_i=k | c_{-i})$---that is, proportional to the
likelihood of $x_i$ given the other data points in cluster $k$ times the prior
probability of $k$ under the Chinese Restaurant Process prior.  The likelihood
for a new category is based on the prior for the category parameters (e.g., mean
and variance).  Once new assignments have been sampled for every $x_i$, the new
values of $c_i$ are one sample from the posterior.  Multiple samples are drawn
by repeatedly sweeping through the data in this way, recording the sampled
assignments for each sweep.

## Online algorithm: Particle filter

In contrast to MCMC algorithms, sequential Monte Carlo (SMC) algorithms do not
require that all data be available simultaneously.  Rather then generating
samples one at a time based on the entire dataset, they maintain a population of
hypotheses (or particles), that are each updated in parallel as the data comes
in.  After $n-1$ observations, particle $j$ has an associated weight
$w^{(j)}_{n-1}$ and clustering $c_{1:n-1}^{(j)}$.  There are many different
strategies for updating particle $j$ based on the the next observation $x_n$.
Here, we follow the approach of @Chen2000 [as described in @Fearnhead2004].
First, as in the Gibbs sampler, an assignment is drawn from $p(c_n=k |
c^{(j)}_{1:n-1}, x_{1:n})$.  Next, the weight is updated to be
$$
    w_{n}^{(j)} = w_{n-1}^{(j)} \times
        \frac{\sum_k p((c_{1:n-1}^{(j)},k) | x_{1:n})}
             {p(c_{1:n-1}^{(j)} | x_{1:n-1})} 
$$
(normalized such that all the new weights sum to 1).  The sum in the numerator
ensures that the new weight reflects the ability of this particle to predict the
actual data point observed, rather than just how well the sampled component
explains it [see @Chen2000; @Fearnhead2004 for further discussion].

One common problem with particle filters is that a small number of particles
capture nearly all the weight, which drastically reduces the effective number of
samples and hence the amount of information about the actual distribution they
are approximating.  In order to prevent this, when the variance of the weights
becomes too high, a rejuvenation step resamples particles (with replacement)
proportional to their weights, and resets the weights to be equal.  The
threshold for the variance of the weights was set to 50% of the mean of the
weights [as suggested by @Fearnhead2004].

# Methods

In order to evaluate the particle filter as a model of phonetic distributional
learning, I applied it to the problem of learning the English distinction
between voiced /b/ and voiceless /p/, based on voice onset time (VOT).  This is
the primary acoustic cue to voicing for word-initial stops in English
[@Lisker1964], and exhibits a clear bimodality.  In order to simulate random VOT
datasets, I fit two normal distributions to the VOTs for /b/ and /p/ from the
Buckeye corpus [@Pitt2007] by @Nelson2017.  This particular corpus shows low
levels of talker variability in VOT [see @Kleinschmidta].  @Fig:vothist shows an
example randomly generated set of VOTs.

```{julia; label="vothist"; fig_cap="Example sample of VOTs used to assess distributional learning models.  With fewer observations, the cluster structure is not obvious, but with more clusters it bnecomes clearer."}

include("VOTs.jl")
using .VOTs

srand(1986)
vot_demo = DataFrame(vot = rand(bp_mix, 10000),
                     n = append!([10], round.(Int, exp10.(ceil.(log10.(2:10000))))))
plot(layer(vot_demo, x=:vot, xgroup=:n, Geom.subplot_grid(Geom.histogram(density=true, bincount=30))),
     # Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]),
     Guide.xlabel("Voice onset time (VOT, ms)"))

```

The Gibbs sampler and particle filter models were implemented in Julia
[@Bezanson2017].[^particlesjl] Each was run on 200 randomly generated data sets
of up to 10,000 observations, collecting intermediate results after 10, 100, and
1,000 observations.  For comparison, 6-month-old infants in @Bergelson2017 heard
on average around 200 tokens of word-initial /b/ and /p/ in a single day, which
would be 60,000 extrapolated to an entire year.  For each dataset, the number of
particles (or samples for the Gibbs sampler) varied (10, 100, and 1,000) and the
concentration parameter $\alpha$ varied (0.01, 0.1, 1, and 10).  The results for
the Gibbs sampler were qualitatively the same across the number of samples and
number of observations, so results are only shown for 1,000 samples (after 500
burnin) and 1,000 observations.

For the prior distribution over cluster means and variances, both used a weakly
informative conjugate Normal-$\chi^{-2}$ prior [@Gelman2003], with $\mu_0$ and
$\sigma^2_0$ set to the overall mean and variance of all the VOTs, and $\kappa_0
= 0.05$ (to allow for significant possible variation in the cluster means) and
$\nu_0 = 2$ (to constrain the variances to reasonable values).

[^particlesjl]: Available online at
    [github.com/kleinschmidt/Particles.jl](https://github.com/kleinschmidt/Particles.jl)

For each run, the approximated posterior distribution of cluster numbers is
recorded.  In the case of the Gibbs sampler, this is simply the proportion of
samples with each number of clusters $K$.  The calculation is analogous for the
particle filter, except that the weights need to be taken into account.  If
particle $j$ has $K^{(j)}$ clusters and weight $w^{(j)}$, then $p(K=k | x_{1:N})
= \sum_j w^{(j)} \delta_{k}(K^{(j)})$ (where $\delta_k(x)$ is the indicator
function, which is 1 if $x=k$ and 0 otherwise).

# Results

A natural measure of success is the probability assigned to $K=2$, since the
data was generated by a two-component mixture (@Fig:pK2).  Additionally,
@Fig:meanK shows the expected number of clusters $E(K | x_{1:N}) = \sum_k k
p(K=k | x_{1:N})$, and @Fig:pK shows the full distributions ($p(K | x_{1:N})$.

```{julia}

# include("Experiments.jl")
# using .Experiments

# I had to make an actual honest-to-god package to make this work with JLD2...
# Pkg.clone("git@github.com:kleinschmidt/Experiments.jl.git")
using Experiments

@load "results/run2-2018-01-30T10:05:12.518.jld2" exs results
@load "results/run1_gibbs_2018-01-30T12:18:31.076.jld2" gibbs_exs gibbs_res

using Bootstrap
bootstrap_ci(v::AbstractVector, f::Function; n=1000) =
    ci(bootstrap(v, f, BasicSampling(n)), BasicConfInt())[1]
function bootstrap_ci(df::AbstractDataFrame, col::Symbol, f::Function; n=1000)
    ci = bootstrap_ci(df[col], f, n=n)
    DataFrame([[c] for c in ci], [col, Symbol("$(col)_low"), Symbol("$(col)_high")])
end

bootstrap_ci(df::AbstractDataFrame, cols::Vector{Symbol}, f::Function; n=1000) =
    reduce(hcat, bootstrap_ci(df, c, f, n=n) for c in cols)

summarize_exp_res(ers) = 
    @_ ers |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    @where(_, (!isnan).(:K_mean)) |>
    @transform(_, p_K_eq_2=get.(:p_of_K, 2, 0)) |>
    by(_, [:num_particle, :num_obs, :α]) do d
        bootstrap_ci(d, [:K_mean, :p_K_eq_2], mean)
    end

run2_ers = flatten(exs, results, :num_obs)
run2_summary = @_ run2_ers |>
    summarize_exp_res |>
    @where(_, :num_particle .> 1) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α)))

gibbs_summary = @_ zip(gibbs_exs, gibbs_res) |>
    summarize_exp_res |>
    @where(_, :num_obs .== 1000, :num_particle .== 1000) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α)))
```

## Gibbs sampler

Since the Gibbs sampler provides the "reference" approximation for these models,
we first examine those results, shown as the gray lines in @Fig:pK2 and
@Fig:meanK.  Of the values tested here, the Gibbs sampler performs best with
$\alpha=0.1$, allocating the majority of the posterior probability to $K=2$.
Nevertheless, the Gibbs sampler maintains substantial uncertainty, allocating
some 15-20%probability to clusterings with more or less than two clusters.  For
$\alpha$ an order of magnitude smaller or larger than this, the Gibbs sampler
infers fewer or more (respectively) clusters than there actually are
(@Fig:meanK), again with some substantial uncertainty.

## Particle filter

The particle filter, unlike the Gibbs sampler, works best when $\alpha=0.01$
(the smallest value tested).  At this value, with a larger number particles,
nearly 100% probability is assigned to the true $K=2$ (first panel,
@Fig:pK2).[^three] For larger values of $\alpha$, it eventually overshoots and
infers more than 2 clusters.  For instance, @Fig:pK2, second panel
($\alpha=0.1$), shows that the probability assigned to $K=2$ clusters rises to a
maximum around 100 data points, but then falls with more data as more complex
clusterings are increasingly preferred (@Fig:pK).  Even though the expected
number of clusters is just slightly more than 2 (@Fig:meanK) as with the Gibbs
sampler, there's no reason to think that it would continue to increase with more
data, since there's no way for the particle filter to go back to simpler
solutions once they've been forgotten.

[^three]: Similar results are obtained for a three-component mixture, modeled
    after the languages described in @Lisker1964 with three voicing categories.
    The primary difference is that more data is required to correctly infer that
    there are three categories for low values of $\alpha$.

Generally, particle filters with more particles better match the Gibbs sampler.
This is not surprising: more particles mean more tolerance for uncertainty,
which means that the particle filter is less committed to particular
classifications that it made in the past.  With few particles, it's possible
(and indeed likely) that most of the particles agree on how the first data
points should be classified, even if there is (ideally) some uncertainty there.
Furthermore, with few particles, the prior has outsized influence.  For low
$\alpha$, the 10-particle filter undershoots the number of clusters inferred by
the Gibbs sampler (and by more particles), while for high $\alpha$ it
overshoots.

<!-- points to make:

* works well for v. small alpha (0.01), and surprisingly quickly.
* particle filter is better approximation (closer to the gibbs sampler) with more
  observations and more particles.
* when there are fewer particles, seem to get exaggerated influence of prior
  (alpha): more bias towards small K for v. low, runs away at higher \alpha.
* except for v. small values of alpha
* problem is that it can't keep the full uncertainty about the clustering "in
  memory", because it can't go back and revise previous classifications (the
  data is gone).  _that might not be a bad thing for psycological
  plausibility!_.  And actually may work in favor for smaller learning rates.
* Also can never go back to simpler solutions once they've fallen out of
  consideration.  This may be why for large \alpha the number of components runs
  away.
* likewise why it works better for small alpha: can't re-visit the decision to
  create a second cluster, but it'll be hard to do it again in the future
  (because prior scales with $\alpha/N$.

 -->

```{julia; label="pK2"; fig_cap="Posterior probability correct (two-cluster solutions) for particle filter (colors, 95% bootstrapped CIs over runs) and Gibbs sampler (gray horizontal lines). Gibbs sampler is shown for 1,000 observations and 1,000 samples."}

Gadfly.push_theme(style(errorbar_cap_length=2pt))

plot(run2_summary, xgroup=:α_str,
     Geom.subplot_grid(Coord.cartesian(ymin=0., ymax=1.),
                       layer(run2_summary,
                             x=:num_obs, y=:p_K_eq_2,
                             ymin=:p_K_eq_2_low, ymax=:p_K_eq_2_high,
                             color=:num_particle, xgroup=:α_str,
                             Geom.line, Geom.point, Geom.errorbar),
                       layer(gibbs_summary,
                             yintercept=:p_K_eq_2, xgroup=:α_str,
                             Geom.hline,
                             Theme(default_color=Gray(0.5)))),
     Scale.x_log10, Scale.color_discrete,
     Guide.ylabel("Average probability of correct (K=2)"),
     Guide.xlabel("Number of observations"),
     Guide.colorkey(title="Particles"),
     Guide.manual_color_key("", ["Gibbs"], [Gray(0.5)]))

```

```{julia; label="meanK"; fig_cap="The expected number of clusters for particle filter (colors, 95% bootstrapped CIs over runs) and Gibbs sampler (gray horizontal lines).  Gibbs sampler is shown for 1,000 observations and 1,000 samples"}

plot(run2_summary, xgroup=:α_str,
     Geom.subplot_grid(layer(run2_summary,
                             x=:num_obs, y=:K_mean,
                             ymin=:K_mean_low, ymax=:K_mean_high,
                             color=:num_particle, xgroup=:α_str,
                             Geom.line, Geom.point, Geom.errorbar),
                       layer(gibbs_summary,
                             yintercept=:K_mean, xgroup=:α_str,
                             Geom.hline,
                             Theme(default_color=Gray(0.5)))),
     Scale.x_log10, Scale.y_log2, Scale.color_discrete,
     Guide.ylabel("Expected number of clusters"),
     Guide.xlabel("Number of observations"),
     Guide.colorkey(title="Particles"),
     Guide.manual_color_key("", ["Gibbs"], [Gray(0.5)]))

```


```{julia; label="pK"; fig_cap="The posterior probability assigned to each possible number of components, as a function of the number of observations, number of particles, and \$\\alpha\$.  Blue vertical lines show the true number of clusters \$K=2\$."}

# plot p(K | x, α) for various values of alpha

function p_of_K(d)
    pks = d[:p_of_K]
    max_k = maximum(length.(pks))
    ps = zeros(Float64, max_k)
    for pk in pks
        ps[1:length(pk)] .+= pk
    end
    DataFrame(K=1:max_k, p_of_K = ps ./ size(d,1))
end

run2_p_of_K = @_ run2_ers |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K) |>
    @where(_, :num_particle .> 1)

gibbs_p_of_K = @_ zip(gibbs_exs, gibbs_res) |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

@_ @where(gibbs_p_of_K, :num_obs .== 1000, :num_particle .== 1000) |>
    @transform(_, num_particle_ = "Gibbs") |>
    vcat(_, @transform(run2_p_of_K, num_particle_ = string.(:num_particle))) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α))) |>
    @where(_, :α .!= 10, :K .< 9) |>
    plot(_, ygroup=:num_particle_, xgroup=:α_str,
         Geom.subplot_grid(layer(_,
                                 x=:K, y=:p_of_K,
                                 ygroup=:num_particle_, xgroup=:α_str,
                                 color=:num_obs,
                                 Geom.line),
                           layer(by(_, [:num_particle_, :α_str], (d)->DataFrame(K=2)),
                                 xintercept = :K,
                                 ygroup=:num_particle_, xgroup=:α_str,
                                 Geom.vline),
                           Guide.xticks(orientation=:horizontal)),
         Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]), 
         Guide.colorkey(title="Observations"), 
         Guide.ylabel("Probability of cluster number \n(by number of particles)",
                      :vertical),
         Guide.xlabel("Number of clusters."))

```

# Discussion

Rational approximations to rational models---like the particle filter explored
here---provide a natural bridge between computational-level and
algorithmic-level, cognitive models [@Sanborn2010].  These techniques are useful
in part because they approximate optimal statistical inference as specified by
the underlying Bayesian model.  But they also allow us to explore---in a formal,
quantitative way---how different kinds of cognitive constraints interact with
the structure of the world to affect human cognition and learning.  In the case
of phonetic distributional learning, there is no shortage of cognitively
plausible models [e.g., @McMurray2009b; @Vallabha2007].  The model explored here
psychologically plausible---it takes one data point at a time and only maintains
a small, finite number of hypothetical clusters--and also grounded in a
computational-level model of the problem of distributional learning and its
statistically optimal solution.

The approximation to the statistically optimal solution to distributional
learning provided by the particle filtering algorithm of @Chen2000 can, in fact,
recover the underlying structure of the particular model system I examined here
(the American English /b/-/p/ contrast).  More importantly, this particular sort
of approximation constrains the model in a similar way that language learners
are constrained: they cannot endlessly re-analyze every single sound they have
ever heard, nor can they maintain an essentially infinite set of hypotheses
about how those sounds should be clustered.  In doing so, it provides some
insight into how people actually solve the statistical problem posed by the
computational-level model of distributional learning.  Forgetting its own
history actually _helps_ the particle filter model when there is a strong bias
towards fewer clusters (low $\alpha$).  The particle filter reliably arrives at
the correct two-cluster structure for low values of $\alpha$, even when the
Gibbs sampler fails to do so, since the Gibbs sampler can continuously
second-guess its decisions to create additional clusters.

The tendency for particle filters to get dug into a particular solution is often
a shortcoming, but in this case it may be a benefit.  It also suggests that such
limited resources may actually be a benefit to human learners as well.  It does
not, however, require that such limitations _change_ with development [as in
@Newport1990] or require or particularly benefit from a specific input [as in
@Elman1993].  In fact, the constraints imposed by limited tolerance for
uncertainty in the particle filter model examined here are in a sense the
_opposite_ of the "less is more" hypothesis: these constraints _allow_ the
learner to impose stronger a priori preference for simple explanations and still
learn, rather than _constituting_ a simplicity preference in and of themselves.
Indeed, one of the advantages of the kind of cognitively constrained rational
models described here is that it allows a principled exploration of the way that
cognitive constraints interact with different kinds of input and structural
assumptions on the part of the learner [@Rohde1999; @Siegelman2015].

Additionally, specifying and exploring such models of human behavior also has
the potential to improve basic computational techniques as well.  We know that
humans do manage to learn the underlying cluster structure from unsupervised
input like this.  The particular ways in which _models_ fail at this task can be
instructive for creating better models.  This may mean taking into account
higher-order structure where it's present, like phonotactic and lexical
regularities [@Feldman2013].  But it might also motivate specific techniques to
get around the difficulty of moving from more complex to less complex
clusterings, like adding the possibility of merging clusters when rejuvenating
particles [analogously to reversible-jump MCMC, @Green1995].

Finally, distributional learning continues even in adult listeners, both in
second language learning [e.g., @Pajak2011a] and in adapting to unfamiliar
talkers [@Clayards2008; @Xie2017a].  The same Bayesian models that have been
applied to distributional learning in acquisition can explain many of the
patterns observed in adaptation [@Kleinschmidt2015b].  Listeners are able to
maintain some uncertainty about how previous tokens ought to be categorized
[@Bicknell], but it is currently unclear how this constrains their ability
to learn from distributional information.  The models explored here could just
as easily be applied to adaptation in adults as acquisition in children.

# Conclusion

Approximately-rational models like the particle filter provide a possible bridge
between computational-level models and psychologically plausible
algorithmic-level cognitive models.  A particle filter model of phonetic
distributional learning is able to learn the underlying cluster structure of the
English /b/-/p/ contrast based only on the distribution of a single cue (VOT).
This shows that it is possible to approximate optimal Bayesian inference in this
domain without making the psychologically-implausible assumptions of batch
processing and unlimited tolerance of uncertainty.  However, the behavior of
this approximation also diverges in potentially interesting ways from a
less-constrained approximate inference model (a Gibbs sampler), suggesting that
the constraints posed by limited cognitive resources are a critical piece of the
puzzle in understanding cognition, even for computational-level modeling
[@Marr1982].
