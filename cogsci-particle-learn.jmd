---
title: "Learning distributions as they come: Particle filter models for online
  distributional learning of phonetic categories"
author: 
- name: "Dave F. Kleinschmidt"
  email: "davidfk@princeton.edu"
  affiliation:
  - "Princeton Neuroscience Institute"
  - "Princeton, NJ 08544 USA"
abstract: >
  Babies must learn the structure of their native language---including the
  number and properties of phonetic categories. Blah BLah.
bibliography: /home/dave/Documents/papers/zotero.bib
---

```{julia; echo=false; results="hide"}

using Weave
Weave.set_chunk_defaults(Dict(:echo=>false,
                              :results=>"hide"))

using Particles
using Underscore

using Distributions, ConjugatePriors, DataFrames, DataFramesMeta, JLD2
using Gadfly

using Colors
ngray(start, finish) = n -> linspace(Gray(start), Gray(finish), n)

```



# Bayesian models of distributional learning

Distributional learning models all attempt to solve the same problem: given some
data points $x_{1:N}$, we wish to know how many clusters generated them (and
what those clusters look like).  We assume that these data were generated from
some number of clusters $K$, where $K$ could be (in principle) anywhere between
1 and $N$ (the number of data points).  A complete clustering of these data
points has two parts: the cluster that each data point is assigned to (denoted
$c_{1:N}$), and the properties of the clusters themselves (which in the example
below are the mean $\mu_k$ and variance $\sigma^2_k$ of a normal distribution,
but in general are the parameters of some probability distribution).  These
models allow for a possibly infinite number of clusters, but have an inductive
bias towards simpler, more parsimonious clusterings.

The first difficulty is that there's no perfect, un-ambiguous solution to this
problem (in general), since clusters are often overlapping and of dfiferent
sizes and frequencies.  For this reason, Bayesian nonparametric models frame
this as a problem of _statistical inference under uncertainty_, and aim to find
the posterior probability of clusterings given some particular data, $p(c_{1:N},
\mu_{1:K}, \sigma^2_{1:K} | x_{1:N})$ [see @Gershman2012 for an excellent
introduction].  In many cases, it's easier to do this in two stages: first,
compute the distribution of cluster assignments $p(c_{1:N} | x_{1:N})$
(marginalizing over possible clusters _parameters_), and then later computing
the distribution over cluster properties _conditional_ on the cluster
assignments $p(\mu_{1:K}, \sigma^2_{1:K} | c_{1:N}, x_{1:N})$.[^conjugate] Here
the focus is on the first part: computing the distribution of cluster
assignments given the data.

[^conjugate]: One reason for this is that often when using a conjugate prior for
    the cluster parameters, there's no need to actually know the exact
    properties of cluster to evaluate how good it is; all that's needed is the
    data points that are assigned to that cluster (and some measure of how
    similar they are to each other).

In principle, computing the posterior distribution $p(c_{1:N} | x_{1:N})$ is a
simple matter of applying Bayes Rule:
$$
    p(c_{1:N} | x_{1:N}) \propto p(x_{1:N} | c_{1:N}) p(c_{1:N})
$$
The first term is the likelihood, or the probability that the observed data is
generated by a particular hypothetical clustering.  The second term is the
prior, or how likely such a clustering is before any data is observed.  The
prior is where inductive biases for simpler clustering can be introduced.  

The particular prior used here is the Dirichlet process.  This prior has a
rich-get-richer structure, which is most easily understood by considering the
conditional prior $p(c_n | c_{1:n-1})$ over cluster assignments for the $n$th
data point, given assignments 1 to $n-1$.  The prior probability of assigning to
a cluster $k$ is
$$
    p(c_n=k | c_{1:n-1}) \propto \left\{
        \begin{array}{ll}
            N_k    & \textrm{existing cluster} \\
            \alpha & \textrm{new cluster}
        \end{array}\right.
$$
where $N_k$ is the the number of other data points assigned to cluster $k$.  The
$\alpha$ parameter is called the concentration parameter and controls how strong
the simplicity bias is.  If $\alpha$ is very low, it's highly unlikely (a
priori) that a new cluster will be created, especially when there are many data
points.  The overall prior probability of any complete clustering $c_{1:N}$ can
be computed from these conditional probabilities under the assumption that the
data is _exchangeable_ (that order doesn't matter, or that the cluster structure
is stable over time), and is proportional to $\prod_{2=1}^{N} p(c_i | c_{1:i-1})
\propto \prod_{k=1}^K \alpha N_k!$ (arbitrarily defining $c_1 = 1$).


The second difficulty is that there are an enormous number of possible
combinations of cluster assignments for $N$ points, $c_{1:N}$.  Even if we know
that there are only $K=2$ clusters, there are still $2^N$ possible ways to
assign $N$ points to two clusters, each of which has some probability associated
with it.  Likewise for every $K$ from 1 to $N$.  Luckily, most of these values
for $c_{1:N}$ have probability that is so small it's essentially zero, and thus
the whole distribution $p(c_{1:N} | x_{1:N})$ can be approximated by a
reasonably small number of _samples_, or hypothetical values of $c_{1:N}$.

## Batch algorithm: Gibbs sampler

One standard method of doing approximate inference by sampling is a Gibbs
sampler, a form of Markov Chain Monte Carlo (MCMC) techniques.  These are named
because they work by sampling (the "Monte Carlo" part) a new value for the
quantity of interest given only the data and the previously sampled value (the
"Markov Chain" part).

For a Dirichlet Process mixture model, this algorithm works by sweeping through
the data, one data point at a time, re-sampling the cluster assignment for that
data point conditioned on the other data points.  If $c_{-i}$ are the cluster
assignments for every point but $x_i$, then the Gibbs sampler will assign $x_i$
to cluster $k$ with probability $p(c_i=k | x_{1:N}, c_{-i}) \propto p(x_i |
c_i=k, x_{-i}, c_{-i}) p(c_i=k | c_{-i})$---that is, proportional to the
likelihood of $x_i$ given the other data points in cluster $k$ times the prior
probability of $k$ under the Chinese Restaurant Process prior.  The likelihood
for a new category is based on the prior for the category parameters (e.g., mean
and variance).  Once new assignments have been sampled for every $x_i$, the new
values of $c_i$ are one sample from the posterior.  Multiple samples are drawn
by repeatedly sweeping through the data in this way, recording the sampled
assignments for each sweep.

## Online algorithm: Particle filter

In contrast to MCMC algorithms, sequential Monte Carlo (SMC) algorithms do not
require that all data be available simultaneously.  Rather then generating
samples one at a time based on the entire dataset, they maintain a population of
hypotheses (or particles), that are each updated in parallel as the data comes
in.  After $n-1$ observations, particle $j$ has an associated weight
$w^{(j)}_{n-1}$ and clustering $c_{1:n-1}^{(j)}$.  There are many different
strategies for updating particle $j$ based on the the next observation $x_n$.
Here, we follow the approach of [@Chen2000; as described in @Fearnhead2004].
First, as in the Gibbs sampler, an assignment is drawn from $p(c_n=k |
c^{(j)}_{1:n-1}, x_{1:n})$.  Next, the weight is updated to be
$$
    w_{n}^{(j)} = w_{n-1}^{(j)} \times
        \frac{\sum_k p((c_{1:n-1}^{(j)},k) | x_{1:n})}
             {p(c_{1:n-1}^{(j)} | x_{1:n-1})} 
$$
(normalized such that all the new weights sum to 1).  The sum in the numerator
ensures that the new weight reflects the ability of this particle to predict the
actual data point observed, rather than just how well the sampled component
explains it.  This helps to maintain more information about the history of
particles, which is a primary consideration when constructing particle filters.

One common problem with particle filters is that a small number of particles
capture nearly all the weight, which drastically reduces the effective number of
samples and hence the amount of information about the actual distribution they
are approximating.  In order to prevent this, when the variance of the weights
becomes too high, a rejuvination step resamples particles (with replacement)
proportional to their weights, and resets the weights to be equal.  The
threshold for the variance of the weights was set to 50% of the mean of the
weights [as suggested by @Fearnhead2004].

# Methods

In order to evaluate the particle filter as a model of phonetic distributional
learning, I applied it to the problem of learning the English distinction
between voiced /b/ and voiceless /p/, based on voice onset time (VOT).  This is
the primary acoustic cue to voicing for word-initial stops in English
[@Lisker1964], and exhibits a clear bimodality.  In order to simulate random VOT
datasets, I fit a normal distribution to the VOTs for /b/ and /p/ from the
Buckeye corpus [@Pitt2007] by @Nelson2017.  This particular corpus shows low
levels of talker variability in VOT [see @Kleinschmidta].  @Fig:vothist shows an
example randomly generated set of VOTs.

```{julia; label="vothist"; fig_cap="Example sample of VOTs used to assess distributional learning models.  With fewer observations, the cluster structure is not obvious, but with more clusters it becomes clearer."}

include("VOTs.jl")
using .VOTs

srand(1986)
vot_demo = DataFrame(vot = rand(bp_mix, 10000),
                     n = append!([10], round.(Int, exp10.(ceil.(log10.(2:10000))))))
plot(layer(vot_demo, x=:vot, xgroup=:n, Geom.subplot_grid(Geom.histogram(density=true, bincount=30))),
     # Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]),
     Guide.xlabel("Voice onset time (VOT, ms)"))

```

The Gibbs sampler and particle filter models were implemented in Julia
[@Bezanson2017], and run on 200 randomly generated data sets.[^particlesjl]  For
each dataset, the number of particles (or samples for the Gibbs sampler), the
number of observations available, and the concentration parameter $\alpha$ were
varied.  The results for the Gibbs sampler were qualitatively the same across
the number of samples and number of observations, so results are only shown for
1,000 samples (500 burnin) and 1,000 observations.

For the prior distribution over cluster means and variances, both used a weakly
informative conjugate Normal-$\chi^{-2}$ prior [@Gelman2003], with $\mu_0$ and
$\sigma^2_0$ set to the overall mean and variance of all the VOTs, and $\kappa_0
= 0.05$ (to allow for significant possible variation in the cluster means) and
$\nu_0 = 2$ (to constrain the variances to reasonable values).

[^particlesjl]: Available online at
    [github.com/kleinschmidt/Particles.jl](https://github.com/kleinschmidt/Particles.jl)

For each run, the approximated posterior distribution of cluster numbers is
recorded.  In the case of the Gibbs sampler, this is simply the proportion of
samples with each number of clusters $K$.  The calculation is analogous for the
particle filter, except that the weights need to be taken into account.  If
particle $j$ has $K^{(j)}$ clusters and weight $w^{(j)}$, then $p(K=k | x_{1:N})
= \sum_j w^{(j)} \delta_{k}(K^{(j)})$ (where $\delta_k(x)$ is the indicator
function, which is 1 if $x=k$ and 0 otherwise).

# Results

A natural measure of success is the probability assigned to $K=2$, since the
data was generated by a two-component mixture (@Fig:pK2).  I also plot the
expected number of clusters $E(K | x_{1:N}) = \sum_k k p(K=k | x_{1:N})$
(@Fig:meanK), as well as the full distributions ($p(K | x_{1:N})$, @Fig:pK).


```{julia}

# include("Experiments.jl")
# using .Experiments

# I had to make an actual honest-to-god package to make this work with JLD2...
# Pkg.clone("git@github.com:kleinschmidt/Experiments.jl.git")
using Experiments

@load "results/run2-2018-01-30T10:05:12.518.jld2" exs results
@load "results/run1_gibbs_2018-01-30T12:18:31.076.jld2" gibbs_exs gibbs_res

run2_ers = flatten(exs, results, :num_obs)
run2_summary = results_summary(run2_ers)

gibbs_summary = @_ zip(gibbs_exs, gibbs_res) |>
    results_summary |>
    @where(_, :num_obs .== 1000, :num_particle .== 1000)
```

## Gibbs sampler

Since the Gibbs sampler provides the "reference" approximation for these models,
we first examine those results, shown as the gray lines in @Fig:pK2 and
@Fig:meanK.  The Gibbs sampler performs best with $\alpha=0.1$, allocating the
majority of the posterior probability to $K=2$.  For $\alpha$ an order of
magnitude smaller or larger than this, the Gibbs sampler infers fewer or more
(respectively) clusters than there actually are (@Fig:meanK).

## Particle filter

The particle filter, unlike the Gibbs sampler, works best when $\alpha=0.01$
(the smallest value tested).  For larger values, it eventually overshoots and
infers more than 2 clusters.  For instance, @Fig:pK2, second panel
($\alpha=0.1$), shows that the probability assigned to $K=2$ clusters rises to a
maximum after 100 data points, but then falls with more data as more complex
clusterings are increasingly preferred (@Fig:pK).

<!-- points to make:

* works well for v. small alpha (0.01), and surprisingly quickly.
* particle filter is better approximation (closer to the gibbs sampler) with more
  observations and more particles.
* when there are fewer particles, seem to get exaggerated influence of prior
  (alpha): more bias towards small K for v. low, runs away at higher \alpha.
* except for v. small values of alpha
* problem is that it can't keep the full uncertainty about the clustering "in
  memory", because it can't go back and revise previous classifications (the
  data is gone).  _that might not be a bad thing for psycological
  plausibility!_.  And actually may work in favor for smaller learning rates.
* Also can never go back to simpler solutions once they've fallen out of
  consideration.  This may be why for large \alpha the number of components runs
  away.
* likewise why it works better for small alpha: can't re-visit the decision to
  create a second cluster, but it'll be hard to do it again in the future
  (because prior scales with $\alpha/N$.

 -->




The particle filter performs best for the smallest value of $\alpha$ tested
($\alpha = 0.01$).  At this level, it assigns nearly 100% probability to
2-cluster solutions when 1,000 particles are used, and nearly 90% (and
increasing) for 100 particles.  For large values of $\alpha \in \{1, 10\}$, the
particle filter (like the Gibbs sampler) vastly over-estimates the number of
clusters (@Fig:meanK).  Even at $\alpha = 0.1$, both the particle filter and the
Gibbs sampler slightly overestimate the number of clusters, assigning some
small probability to $K=3$.

```{julia; label="pK2"; fig_cap="Posterior probability correct (two-cluster solutions) for particle filter (colors) and Gibbs sampler (gray horizontal lines). Gibbs sampler is shown for 1,000 observations and 1,000 samples."}

plot(run2_summary, xgroup=:α,
     Geom.subplot_grid(layer(run2_summary, 
                             x=:num_obs, y=:avg_p_K2, color=:num_particle, xgroup=:α,
                             Geom.line, Geom.point),
                       layer(gibbs_summary,
                             yintercept=:avg_p_K2, xgroup=:α,
                             Geom.hline,
                             Theme(default_color=Gray(0.5)))),
     Scale.x_log10, Scale.color_discrete,
     Guide.ylabel("Average probability of correct (K=2)"),
     Guide.xlabel("Number of observations"),
     Guide.colorkey(title="Particles"),
     Guide.manual_color_key("", ["Gibbs"], [Gray(0.5)]))

```

```{julia; label="meanK"; fig_cap="The expected number of clusters for particle filter (colors) and Gibbs sampler (gray horizontal lines).  Gibbs sampler is shown for 1,000 observations and 1,000 samples"}

plot(run2_summary, xgroup=:α,
     Geom.subplot_grid(layer(run2_summary, 
                             x=:num_obs, y=:avg_K_mean, color=:num_particle, xgroup=:α,
                             Geom.line, Geom.point),
                       layer(gibbs_summary,
                             yintercept=:avg_K_mean, xgroup=:α,
                             Geom.hline,
                             Theme(default_color=Gray(0.5)))),
     Scale.x_log10, Scale.y_log2, Scale.color_discrete,
     Guide.ylabel("Expected number of clusters"),
     Guide.xlabel("Number of observations"),
     Guide.colorkey(title="Particles"),
     Guide.manual_color_key("", ["Gibbs"], [Gray(0.5)]))

```



```{julia; label="pK"; fig_cap="The posterior probability assigned to each possible number of components, as a function of the number of observations, number of particles, and α"}

# plot p(K | x, α) for various values of alpha

function p_of_K(d)
    pks = d[:p_of_K]
    max_k = maximum(length.(pks))
    ps = zeros(Float64, max_k)
    for pk in pks
        ps[1:length(pk)] .+= pk
    end
    DataFrame(K=1:max_k, p_of_K = ps ./ size(d,1))
end

run2_p_of_K = @_ run2_ers |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

gibbs_p_of_K = @_ zip(gibbs_exs, gibbs_res) |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

@_ @where(gibbs_p_of_K, :num_obs .== 1000, :num_particle .== 1000) |>
    @transform(_, num_particle_ = "Gibbs") |>
    vcat(_, @transform(run2_p_of_K, num_particle_ = string.(:num_particle))) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α))) |>
    @where(_, :num_particle_ .!= "1", :α .!= 10, :K .< 9) |>
    plot(_, x=:K, y=:p_of_K, ygroup=:num_particle_, xgroup=:α_str, color=:num_obs,
         Geom.subplot_grid(Geom.line, Guide.xticks(orientation=:horizontal)), 
         Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]), 
         Guide.colorkey(title="Observations"), 
         Guide.ylabel("Probability of cluster number \n(by number of particles)", :vertical))

```
