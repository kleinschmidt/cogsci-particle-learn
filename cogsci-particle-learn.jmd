---
title: "Learning distributions as they come: Particle filter models for online
  distributional learning of phonetic categories"
author: 
- name: "Dave F. Kleinschmidt"
  email: "davidfk@princeton.edu"
  affiliation:
  - "Princeton Neuroscience Institute"
  - "Princeton, NJ 08544 USA"
abstract: >
  Babies must learn the structure of their native language---including the
  number and properties of phonetic categories. Blah BLah.
bibliography: /home/dave/Documents/papers/zotero.bib
---

```{julia; echo=false; results="hide"}

using Weave
Weave.set_chunk_defaults(Dict(:echo=>false,
                              :results=>"hide"))

using Particles
using Underscore

using Distributions, ConjugatePriors, DataFrames, DataFramesMeta, JLD2
using Gadfly

using Colors
ngray(start, finish) = n -> linspace(Gray(start), Gray(finish), n)

```



# Bayesian models of distributional learning

Bayesian models of distributional learning formulate it as a problem of
_statistical inference_: given some data points $x_{1:N}$, we wish to know how
many clusters generated them (and what those clusters look like).  We assume
that these data were generated from some number of clusters $K$, where $K$ could
be (in principle) anywhere between 1 and $N$ (the number of data points).  A
complete clustering of these data points has two parts: the cluster that each
data point is assigned to (denoted $c_{1:N}$), and the properties of the
clusters themselves (which in the example below are the mean $\mu_k$ and
variance $\sigma^2_k$ of a normal distribution, but in general are the
parameters of some probability distribution).  These models allow for a possibly
infinite number of clusters, but have an inductive bias towards simpler, more
parsimonious clusterings.

The first difficulty is that there's no perfect, un-ambiguous solution to this
problem (in general), since clusters are often overlapping and of dfiferent
sizes and frequencies.  Rather, Bayesian nonparametric models aim to find the
posterior probability of clusterings given some particular data, $p(c_{1:N},
\mu_{1:K}, \sigma^2_{1:K} | x_{1:N})$.  In many cases, it's easier to do this in
two stages: first, compute the distribution of cluster assignments $p(c_{1:N} |
x_{1:N})$ (marginalizing over possible clusters _parameters_), and then later
computing the distribution over cluster properties _conditional_ on the cluster
assignments $p(\mu_{1:K}, \sigma^2_{1:K} | c_{1:N}, x_{1:N})$.[^conjugate]  Here
the focus is on the first part: computing the distribution of cluster
assignments given the data.

[^conjugate]: One reason for this is that when using a conjugate prior for the
    cluster parameters, there's no need to actually know the exact properties of
    cluster to evaluate how good it is; all that's needed is the data points
    that are assigned to that cluster (and some measure of how similar they are
    to each other).

In principle, computing the posterior distribution $p(c_{1:N} | x_{1:N})$ is a
simple matter of applying Bayes Rule:
$$
    p(c_{1:N} | x_{1:N}) \propto p(x_{1:N} | c_{1:N}) p(c_{1:N})
$$
The first term is the likelihood, or the probability that the observed data is
generated by a particular hypothetical clustering.  The second term is the
prior, or how likely such a clustering is before any data is observed.

The second difficulty is that there are an enormous number of possible
combinations of cluster assignments for $N$ points, $c_{1:N}$.  Even if we know
that there are only $K=2$ clusters, there are still $2^N$ possible ways to
assign $N$ points to two clusters, each of which has some probability associated
with it.  Likewise for every $K$ from 1 to $N$.  Luckily, most of these values
for $c_{1:N}$ have probability that is so small it's essentially zero, and thus
the whole distribution $p(c_{1:N} | x_{1:N})$ can be approximated by a
reasonably small number of _samples_, or hypothetical values of $c_{1:N}$.

## Batch algorithm: Gibbs sampler

## Online algorithm: Particle filter

# Methods

In order to evaluate the particle filter as a model of phonetic distributional
learning, I applied it to the problem of learning the English distinction
between voiced /b/ and voiceless /p/, based on voice onset time (VOT).  This is
the primary acoustic cue to voicing for word-initial stops in English
[@Lisker1964], and exhibits a clear bimodality.  In order to simulate random VOT
datasets, I fit a normal distribution to the VOTs for /b/ and /p/ from the
Buckeye corpus [@Pitt2007] by @Nelson2017.  This particular corpus shows low
levels of talker variability in VOT [see @Kleinschmidta].  

```{julia; fig_cap="Example sample of VOTs used to assess distributional learning models"}

include("VOTs.jl")
using .VOTs

srand(1986)
plot(layer(x=rand(bp_mix, 10000), Geom.histogram(density=true, bincount=30)),
     Guide.xlabel("Voice onset time (VOT, ms)"))           

```


```{julia}

# include("Experiments.jl")
# using .Experiments

# I had to make an actual honest-to-god package to make this work with JLD2...
using Experiments

using FileIO

# exs, results = load("results/run2-2018-01-30T10:05:12.518.jld2", "exs", "results")
# gibbs_exs, gibbs_res = load("results/run1_gibbs_2018-01-30T12:18:31.076.jld2", "gibbs_exs", "gibbs_res")

@load "results/run2-2018-01-30T10:05:12.518.jld2" exs results
@load "results/run1_gibbs_2018-01-30T12:18:31.076.jld2" gibbs_exs gibbs_res

run2_ers = flatten(exs, results, :num_obs)
run2_summary = results_summary(run2_ers)

gibbs_summary = results_summary(zip(gibbs_exs, gibbs_res))

```

```{julia; label="pK2"; fig_cap="One measure of success is the posterior probability of two-cluster solutions."}
plot(run2_summary, x=:num_obs, y=:avg_p_K2, color=:num_particle, xgroup=:α,
     Geom.subplot_grid(Geom.line, Geom.point), Scale.x_log10, Scale.color_discrete)
```


```{julia; label="meanK"; fig_cap="The expected number of clusters, averaged over runs."}
plot(run2_summary, x=:num_obs, y=:avg_K_mean, color=:num_particle, xgroup=:α,
     Geom.subplot_grid(Geom.line, Geom.point), Scale.x_log10, Scale.y_log2, Scale.color_discrete)
```



```{julia; label="pK"; fig_cap="The posterior probability assigned to each possible number of components, as a function of the number of observations, number of particles, and α"}

# plot p(K | x, α) for various values of alpha

function p_of_K(d)
    pks = d[:p_of_K]
    max_k = maximum(length.(pks))
    ps = zeros(Float64, max_k)
    for pk in pks
        ps[1:length(pk)] .+= pk
    end
    DataFrame(K=1:max_k, p_of_K = ps ./ size(d,1))
end

run2_p_of_K = @_ run2_ers |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

gibbs_p_of_K = @_ zip(gibbs_exs, gibbs_res) |>
    [Dict(er...) for er in _] |>
    dictofarrays |>
    DataFrame |>
    by(_, [:num_particle, :num_obs, :α], p_of_K)

@_ @where(gibbs_p_of_K, :num_obs .== 1000, :num_particle .== 1000) |>
    @transform(_, num_particle_ = "Gibbs") |>
    vcat(_, @transform(run2_p_of_K, num_particle_ = string.(:num_particle))) |>
    @transform(_, α_str = ifelse.(:α .== 0.01, "α = 0.01", string.(:α))) |>
    @where(_, :num_particle_ .!= "1", :α .!= 10, :K .< 9) |>
    plot(_, x=:K, y=:p_of_K, ygroup=:num_particle_, xgroup=:α_str, color=:num_obs,
         Geom.subplot_grid(Geom.line, Guide.xticks(orientation=:horizontal)), 
         Scale.color_discrete(ngray(0.8, 0.0), levels=[10, 100, 1_000, 10_000]), 
         Guide.colorkey(title="Observations"), 
         Guide.ylabel("Probability of cluster number \n(by number of particles)", :vertical))

```
